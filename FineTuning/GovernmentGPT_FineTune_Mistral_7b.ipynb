{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyuOCYM92LJb"
      },
      "source": [
        "# GovernmentGPT: Mistral 7b fine-tuning\n",
        "\n",
        "We wanted to see whether we can teach an LLM to do the job of elected British Members of Parliament (MPs) and debate any issue like they do in the House of Commons.\n",
        "\n",
        "GovernmentGPT is an LLM fine-tuned with a LoRA adapter. You can see the code for this here: https://github.com/stewhsource/GovernmentGPT/FineTuning You can skip this bit and jump straight to inference here: https://github.com/stewhsource/GovernmentGPT/blob/main/Inference/GovernmentGPT_Inference.ipynb\n",
        "\n",
        "This notebook allows you to recreate the GovernmentGPT model with LoRA fine-tuning of Mistral 7b. We use the [Mistral 7b v0.3 base model](https://huggingface.co/mistralai/Mistral-7B-v0.3).\n",
        "\n",
        "The code to recreate the training dataset is available at: https://github.com/stewhsource/GovernmentGPT/tree/main/DatasetPreparation You can also download the fine-tuning training data directly from: https://github.com/stewhsource/GovernmentGPT/tree/main/DatasetPreparation/FineTuningDatasets\n",
        "\n",
        "LLM fine-tuning is computationally very heavyweight, so this notebook needs to be run on a machine with a GPU and a lot of memory. Google Colab provides this in the cloud quickly and easily with their A100 High RAM instances (you'll likely need Colab Pro for that).\n",
        "\n",
        "The fine-tuning approach here is based on the [`mistral-finetune`](https://github.com/mistralai/mistral-finetune/) Git repo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxr8mv-17GfB"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Clone the `mistral-finetune` repo:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIj3IlIeVDIb",
        "outputId": "800778c7-4a62-466f-8f6b-a44e1df54f0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'mistral-finetune'...\n",
            "remote: Enumerating objects: 401, done.\u001b[K\n",
            "remote: Counting objects: 100% (142/142), done.\u001b[K\n",
            "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "remote: Total 401 (delta 125), reused 94 (delta 94), pack-reused 259\u001b[K\n",
            "Receiving objects: 100% (401/401), 210.17 KiB | 5.25 MiB/s, done.\n",
            "Resolving deltas: 100% (209/209), done.\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/mistralai/mistral-finetune.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQPd_pGT7WiY"
      },
      "source": [
        "Install all required dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuTOGipl7BS7",
        "outputId": "8f2c708a-915d-449d-a7fe-d47496751755"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fire (from -r /content/mistral-finetune/requirements.txt (line 1))\n",
            "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: simple-parsing in /usr/local/lib/python3.10/dist-packages (from -r /content/mistral-finetune/requirements.txt (line 2)) (0.1.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from -r /content/mistral-finetune/requirements.txt (line 3)) (6.0.1)\n",
            "Collecting mistral-common>=1.1.0 (from -r /content/mistral-finetune/requirements.txt (line 4))\n",
            "  Downloading mistral_common-1.2.1-py3-none-any.whl (704 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m704.9/704.9 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from -r /content/mistral-finetune/requirements.txt (line 5)) (0.4.3)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r /content/mistral-finetune/requirements.txt (line 6)) (2.15.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r /content/mistral-finetune/requirements.txt (line 7)) (4.66.4)\n",
            "Collecting torch==2.2 (from -r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.2 (from -r /content/mistral-finetune/requirements.txt (line 10))\n",
            "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xformers==0.0.24 (from -r /content/mistral-finetune/requirements.txt (line 11))\n",
            "  Downloading xformers-0.0.24-cp310-cp310-manylinux2014_x86_64.whl (218.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.2/218.2 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9)) (3.15.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9)) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9)) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers==0.0.24->-r /content/mistral-finetune/requirements.txt (line 11)) (1.25.2)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->-r /content/mistral-finetune/requirements.txt (line 1)) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->-r /content/mistral-finetune/requirements.txt (line 1)) (2.4.0)\n",
            "Requirement already satisfied: docstring-parser~=0.15 in /usr/local/lib/python3.10/dist-packages (from simple-parsing->-r /content/mistral-finetune/requirements.txt (line 2)) (0.16)\n",
            "Collecting jsonschema==4.21.1 (from mistral-common>=1.1.0->-r /content/mistral-finetune/requirements.txt (line 4))\n",
            "  Downloading jsonschema-4.21.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic==2.6.1 (from mistral-common>=1.1.0->-r /content/mistral-finetune/requirements.txt (line 4))\n",
            "  Downloading pydantic-2.6.1-py3-none-any.whl (394 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.8/394.8 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece==0.1.99 in /usr/local/lib/python3.10/dist-packages (from mistral-common>=1.1.0->-r /content/mistral-finetune/requirements.txt (line 4)) (0.1.99)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.21.1->mistral-common>=1.1.0->-r /content/mistral-finetune/requirements.txt (line 4)) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.21.1->mistral-common>=1.1.0->-r /content/mistral-finetune/requirements.txt (line 4)) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.21.1->mistral-common>=1.1.0->-r /content/mistral-finetune/requirements.txt (line 4)) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.21.1->mistral-common>=1.1.0->-r /content/mistral-finetune/requirements.txt (line 4)) (0.18.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic==2.6.1->mistral-common>=1.1.0->-r /content/mistral-finetune/requirements.txt (line 4)) (0.7.0)\n",
            "Collecting pydantic-core==2.16.2 (from pydantic==2.6.1->mistral-common>=1.1.0->-r /content/mistral-finetune/requirements.txt (line 4))\n",
            "  Downloading pydantic_core-2.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (1.64.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2->-r /content/mistral-finetune/requirements.txt (line 9)) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->-r /content/mistral-finetune/requirements.txt (line 6)) (3.2.2)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117029 sha256=b3c82bc04a3978394657737ed806370a78069c479b1b611e5b72697abda521ff\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n",
            "Successfully built fire\n",
            "Installing collected packages: triton, pydantic-core, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fire, pydantic, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, jsonschema, torch, mistral-common, xformers\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.0\n",
            "    Uninstalling triton-2.3.0:\n",
            "      Successfully uninstalled triton-2.3.0\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.18.4\n",
            "    Uninstalling pydantic_core-2.18.4:\n",
            "      Successfully uninstalled pydantic_core-2.18.4\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.7.4\n",
            "    Uninstalling pydantic-2.7.4:\n",
            "      Successfully uninstalled pydantic-2.7.4\n",
            "  Attempting uninstall: jsonschema\n",
            "    Found existing installation: jsonschema 4.19.2\n",
            "    Uninstalling jsonschema-4.19.2:\n",
            "      Successfully uninstalled jsonschema-4.19.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.0+cu121\n",
            "    Uninstalling torch-2.3.0+cu121:\n",
            "      Successfully uninstalled torch-2.3.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.3.0+cu121 requires torch==2.3.0, but you have torch 2.2.0 which is incompatible.\n",
            "torchtext 0.18.0 requires torch>=2.3.0, but you have torch 2.2.0 which is incompatible.\n",
            "torchvision 0.18.0+cu121 requires torch==2.3.0, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fire-0.6.0 jsonschema-4.21.1 mistral-common-1.2.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 pydantic-2.6.1 pydantic-core-2.16.2 torch-2.2.0 triton-2.2.0 xformers-0.0.24\n"
          ]
        }
      ],
      "source": [
        "!pip install -r /content/mistral-finetune/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgdIAi257jLo"
      },
      "source": [
        "## Mistral 7B model download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cdl_R5baUyha"
      },
      "outputs": [],
      "source": [
        "#!wget https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-v0.3.tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IgJWR-fReilz"
      },
      "outputs": [],
      "source": [
        "#!DIR=/content/mistral_models && mkdir -p $DIR && tar -xf mistral-7B-v0.3.tar -C $DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384,
          "referenced_widgets": [
            "973c4bb71e3544ad973420c227d16947",
            "ab530c0b4ad9413d85ccb26847c6c9e9",
            "70188b6e3bd74364a7ee078f93cff20d",
            "74240e21e41b4c52aa70b69d1df9fdea",
            "54b6bf2b07904fd99bb657abe69c0227",
            "b9593542b26c499ba1b08abe607207eb",
            "065ebf7c80e8421d8c272a682e6a5dc4",
            "7b07209006a7426aba2640b0349fa55d",
            "ebdfe4731ea54a679a587cacc754cba3",
            "75885cfae9b94b2295bed7d91ff90437",
            "fcea1cd754e6416489b9db6cb6996aff",
            "3893d50398544a46bcf3e4ae0b7290ae",
            "9f44dedd590246abb6ca5dc21771f42b",
            "44b4745d2b45465b8b1f412224e57dba",
            "cba14c8db6e8435d9bd67a2de1f51d29",
            "2d515e5acfe24ab1945ffbd76d2488c8",
            "bc3fc9cc3be3453396bd1d59db8ad675",
            "a670a9bd66fa4e69a484902ec39fb044",
            "70546d1d5c4b42088d6fbea2972f7aa5",
            "afe31d4282cf44afb95273d3b95f3fed",
            "c043b5888f144945be2da6512e798ec0",
            "1a7928bb7a894f5cb0ae0f726d89de0f",
            "05e575b33c614951b1cde79860bcdfc7",
            "1a7810fecfde4b7b87430f4b2a86de86",
            "810e9d9bcf574cbb82c83a34445df56b",
            "76ceb6c3812347f3a7e873acef9b33ff",
            "3c26a3fe6ddc46aaa7ae817b83f0cf25",
            "24b023d1fb5f47d4b382ba15be216ac1",
            "3de279dcc04a4d35bd61e3d6ec32399a",
            "625a865cc43a45028fb818acc986a216",
            "2e627687b8454e12a3a0d60d317bf362",
            "c901cfec744c42a0a204c54c3a2ab4ef",
            "9d634e0998d5467ead1ccf940bdad0d0",
            "fba8f0da949a4feda5be6fe27f6a5437",
            "a495b56297eb4f6ea2cbeaadceb05d5c",
            "4e6ee6e0bb534e67a6661f6286144549",
            "dd9b83a9288d473c8633249dc9ff9696",
            "1cf226bc325943848221e78913309eaa",
            "33d177d4058b409a8b6180a2cf4bbb03",
            "1746029a40ac4a1395cdac05c9b0d229",
            "de9654df2a5c44428ee00c0fa42c0073",
            "de636eb04bc44e718c2f77c74e9c4e99",
            "fde0c9d111064c76a197f990d40e5b4b",
            "e69a3182bd7145229f9b340fffecbd4e"
          ]
        },
        "id": "qgjAADBFHB0S",
        "outputId": "03f02c69-87c1-4632-e97f-3cb60b107643"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/mistral_models/7B-v0.3’: No such file or directory\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.15.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.6.2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "973c4bb71e3544ad973420c227d16947"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model.v3:   0%|          | 0.00/587k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3893d50398544a46bcf3e4ae0b7290ae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "params.json:   0%|          | 0.00/202 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "05e575b33c614951b1cde79860bcdfc7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "consolidated.safetensors:   0%|          | 0.00/14.5G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fba8f0da949a4feda5be6fe27f6a5437"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/mistral_models/7B-v0.3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Alternatively, you can download the model from Hugging Face\n",
        "# (sometimes this is needed as the Mistral mirror is super slow from Colab?)\n",
        "\n",
        "!mkdir /content/mistral_models/7B-v0.3\n",
        "\n",
        "!pip install huggingface_hub\n",
        "from huggingface_hub import snapshot_download\n",
        "from pathlib import Path\n",
        "\n",
        "mistral_models_path = Path.home().joinpath('content','mistral_models', '7B-v0.3')\n",
        "mistral_models_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Import Colab Secrets userdata module\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set HuggingFace API key\n",
        "import os\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "\n",
        "snapshot_download(repo_id=\"mistralai/Mistral-7B-v0.3\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tokenizer.model.v3\"], local_dir='/content/mistral_models/7B-v0.3')\n",
        "\n",
        "#! cp -r /root/mistral_models/7B-v0.3 /content/mistral_models\n",
        "#! rm -r /root/mistral_models/7B-v0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PxYGmcy4gu0",
        "outputId": "ff59932d-bf2e-425f-acae-7425ea78217a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7B-v0.3\n"
          ]
        }
      ],
      "source": [
        "!ls /content/mistral_models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHmcqiFR462E"
      },
      "source": [
        "# Prepare fine-tuning data\n",
        "Use ProduceFineTuningDataset.py separately to produce the datasets. You can then wget or upload manually into data/* here, and ensure the fine-tuning config is pointing to them below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uP7J-Lv75rEp",
        "outputId": "6f02654d-41e2-4fde-c69b-94cb24c53e83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "\n",
        "# make a new directory called data\n",
        "!mkdir -p data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJEqAP-Y5a2g",
        "outputId": "5ac4e220-68bc-4833-bceb-653da0c0de56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/data\n",
            "--2024-06-26 12:21:28--  https://stewh-publicdata.s3.eu-west-2.amazonaws.com/governmentgpt/2024-06-07/datasets/HansardSequences_250k.big.txt.zip\n",
            "Resolving stewh-publicdata.s3.eu-west-2.amazonaws.com (stewh-publicdata.s3.eu-west-2.amazonaws.com)... 52.95.143.106, 52.95.149.10, 3.5.245.108, ...\n",
            "Connecting to stewh-publicdata.s3.eu-west-2.amazonaws.com (stewh-publicdata.s3.eu-west-2.amazonaws.com)|52.95.143.106|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1813919094 (1.7G) [application/zip]\n",
            "Saving to: ‘/content/data/HansardSequences_250k.big.txt.zip’\n",
            "\n",
            "/content/data/Hansa 100%[===================>]   1.69G  15.3MB/s    in 85s     \n",
            "\n",
            "2024-06-26 12:22:54 (20.3 MB/s) - ‘/content/data/HansardSequences_250k.big.txt.zip’ saved [1813919094/1813919094]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%cd /content/data\n",
        "!wget -O /content/data/HansardSequences_250k.big.txt.zip https://stewh-publicdata.s3.eu-west-2.amazonaws.com/governmentgpt/2024-06-07/datasets/HansardSequences_250k.big.txt.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip\n",
        "!unzip /content/data/HansardSequences_250k.big.txt.zip -d /content/data/\n",
        "%cd /content/data/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzndhCX3eiR0",
        "outputId": "6ee59cd0-036d-4e6d-f2c2-74a9d4f463ba"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/data/HansardSequences_250k.big.txt.zip\n",
            "  inflating: /content/data/HansardSequences_250k.big.txt  \n",
            "/content/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1N85aLc2xmw"
      },
      "source": [
        "## Prepare fine-tuning configuration\n",
        "Create the yaml configuration for GovermentGPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hg1nU3oP4dGD",
        "outputId": "919254d3-d501-43d8-a6fb-aac90c9103fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "\n",
        "# make a new directory called config\n",
        "!mkdir -p config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "yyxLt3884ZWV"
      },
      "outputs": [],
      "source": [
        "# define training configuration\n",
        "# for your own use cases, you might want to change the data paths, model path, run_dir, and other hyperparameters\n",
        "\n",
        "config = \"\"\"\n",
        "# data\n",
        "data:\n",
        "  instruct_data: \"\"  # Fill\n",
        "  data: \"/content/data/HansardSequences_250k.big.txt\"  # Optionally fill with pretraining data\n",
        "  eval_instruct_data: \"\"  # Optionally fill\n",
        "\n",
        "# model\n",
        "model_id_or_path: \"/content/mistral_models/7B-v0.3\"  # Change to downloaded path\n",
        "lora:\n",
        "  rank: 64\n",
        "\n",
        "# optim\n",
        "# tokens per training steps = batch_size x num_GPUs x seq_len\n",
        "# we recommend sequence lentgh of 32768\n",
        "# If you run into memory error, you can try reduce the sequence length\n",
        "seq_len: 8192\n",
        "batch_size: 1\n",
        "num_microbatches: 8\n",
        "max_steps: 100\n",
        "optim:\n",
        "  lr: 1.e-4\n",
        "  weight_decay: 0.1\n",
        "  pct_start: 0.05\n",
        "\n",
        "# other\n",
        "seed: 0\n",
        "log_freq: 1\n",
        "eval_freq: 100\n",
        "no_eval: True\n",
        "ckpt_freq: 100\n",
        "\n",
        "save_adapters: True  # save only trained LoRA adapters. Set to `False` to merge LoRA adapter into the base model and save full fine-tuned model\n",
        "\n",
        "run_dir: \"/content/governmentgpt\"\n",
        "\"\"\"\n",
        "\n",
        "# save the same file locally into the example.yaml file\n",
        "import yaml\n",
        "with open('/content/config/governmentgpt.yaml', 'w') as file:\n",
        "    yaml.dump(yaml.safe_load(config), file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ams-19wF8zgY"
      },
      "source": [
        "#Verify data\n",
        "\n",
        "To ensure effective training, mistral-finetune has strict requirements for how the training data has to be formatted. Check out the required data formatting [here](https://github.com/mistralai/mistral-finetune/tree/main?tab=readme-ov-file#prepare-dataset).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIK0VFXHIn8r",
        "outputId": "d71fb27a-bf0e-479c-f216-97ba85136f8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mistral-finetune\n"
          ]
        }
      ],
      "source": [
        "# navigate to the mistral-finetune directory\n",
        "%cd /content/mistral-finetune/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLHNxpN4GS3i"
      },
      "outputs": [],
      "source": [
        "# check training data stats (this causes runtime issues on colab - I think because of the output volume?)\n",
        "# !python -m utils.validate_data --train_yaml /content/config/governmentgpt.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hia7n0T1_mHZ"
      },
      "source": [
        "## Start training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZtcLerooWFeB"
      },
      "outputs": [],
      "source": [
        "# these info is needed for training\n",
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ErD1ktQUMyPZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1870b95-38bb-44d0-d272-7bdf860d1f56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/governmentgpt': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# make sure the run_dir has not been created before\n",
        "# only run this when you ran torchrun previously and created the /content/governmentgpt file\n",
        "! rm -r /content/governmentgpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "JdLqO0h5opUC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4wFgmwIUTtg",
        "outputId": "71e9d12f-9442-4755-dc5a-3398ccfe2479"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mistral-finetune\n",
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
            "0it [00:00, ?it/s]\n",
            "2024-06-26 12:32:12.561247: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-06-26 12:32:12.612767: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-06-26 12:32:12.612826: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-06-26 12:32:12.614428: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-06-26 12:32:12.622296: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-06-26 12:32:13.742897: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "args: TrainArgs(data=DataArgs(data='/content/data/HansardSequences_250k.big.txt', shuffle=False, instruct_data='', eval_instruct_data='', instruct=InstructArgs(shuffle=True, dynamic_chunk_fn_call=True)), model_id_or_path='/content/mistral_models/7B-v0.3', run_dir='/content/governmentgpt', optim=OptimArgs(lr=0.0001, weight_decay=0.1, pct_start=0.05), seed=0, num_microbatches=8, seq_len=8192, batch_size=1, max_norm=1.0, max_steps=100, log_freq=1, ckpt_freq=100, save_adapters=True, no_ckpt=False, num_ckpt_keep=3, eval_freq=100, no_eval=True, checkpoint=True, world_size=1, wandb=WandbArgs(project=None, offline=False, key=None, run_name=None), mlflow=MLFlowArgs(tracking_uri=None, experiment_name=None), lora=LoraArgs(enable=True, rank=64, dropout=0.0, scaling=2.0))\n",
            "2024-06-26 12:32:14 (UTC) - 0:00:06 - distributed - INFO - torch.cuda.device_count: 1\n",
            "2024-06-26 12:32:14 (UTC) - 0:00:06 - distributed - INFO - CUDA_VISIBLE_DEVICES: 0\n",
            "2024-06-26 12:32:14 (UTC) - 0:00:06 - distributed - INFO - local rank: 0\n",
            "2024-06-26 12:32:14 (UTC) - 0:00:06 - train - INFO - Going to init comms...\n",
            "2024-06-26 12:32:14 (UTC) - 0:00:06 - train - INFO - Run dir: /content/governmentgpt\n",
            "2024-06-26 12:32:15 (UTC) - 0:00:06 - train - INFO - TrainArgs: {'batch_size': 1,\n",
            " 'checkpoint': True,\n",
            " 'ckpt_freq': 100,\n",
            " 'data': {'data': '/content/data/HansardSequences_250k.big.txt',\n",
            "          'eval_instruct_data': '',\n",
            "          'instruct': {'dynamic_chunk_fn_call': True, 'shuffle': True},\n",
            "          'instruct_data': '',\n",
            "          'shuffle': False},\n",
            " 'eval_freq': 100,\n",
            " 'log_freq': 1,\n",
            " 'lora': {'dropout': 0.0, 'enable': True, 'rank': 64, 'scaling': 2.0},\n",
            " 'max_norm': 1.0,\n",
            " 'max_steps': 100,\n",
            " 'mlflow': {'experiment_name': None, 'tracking_uri': None},\n",
            " 'model_id_or_path': '/content/mistral_models/7B-v0.3',\n",
            " 'no_ckpt': False,\n",
            " 'no_eval': True,\n",
            " 'num_ckpt_keep': 3,\n",
            " 'num_microbatches': 8,\n",
            " 'optim': {'lr': 0.0001, 'pct_start': 0.05, 'weight_decay': 0.1},\n",
            " 'run_dir': '/content/governmentgpt',\n",
            " 'save_adapters': True,\n",
            " 'seed': 0,\n",
            " 'seq_len': 8192,\n",
            " 'wandb': {'key': None, 'offline': False, 'project': None, 'run_name': None},\n",
            " 'world_size': 1}\n",
            "2024-06-26 12:32:15 (UTC) - 0:00:07 - finetune.wrapped_model - INFO - Reloading model from /content/mistral_models/7B-v0.3/consolidated.safetensors ...\n",
            "2024-06-26 12:32:15 (UTC) - 0:00:07 - finetune.wrapped_model - INFO - Converting model to dtype torch.bfloat16 ...\n",
            "2024-06-26 12:32:15 (UTC) - 0:00:07 - finetune.wrapped_model - INFO - Loaded model on cpu!\n",
            "2024-06-26 12:32:15 (UTC) - 0:00:07 - finetune.wrapped_model - INFO - Initializing lora layers ...\n",
            "2024-06-26 12:32:16 (UTC) - 0:00:08 - finetune.wrapped_model - INFO - Finished initialization!\n",
            "2024-06-26 12:32:16 (UTC) - 0:00:08 - finetune.wrapped_model - INFO - Sharding model over 1 GPUs ...\n",
            "2024-06-26 12:32:21 (UTC) - 0:00:12 - finetune.wrapped_model - INFO - Model sharded!\n",
            "2024-06-26 12:32:21 (UTC) - 0:00:12 - finetune.wrapped_model - INFO - 167,772,160 out of 7,415,795,712 parameters are finetuned (2.26%).\n",
            "2024-06-26 12:32:21 (UTC) - 0:00:13 - dataset - INFO - Lazily loading /content/data/HansardSequences_250k.big.txt ...\n",
            "2024-06-26 12:32:42 (UTC) - 0:00:33 - train - INFO - step: 000001 - done (%): 1.0 - loss: 1.377 - lr: 4.0e-06 - peak_alloc_mem (GB): 21.0 - alloc_mem (GB): 17.1 - words_per_second: 3200.8 - avg_words_per_second: 3200.8 - ETA: >2024-06-26 13:06:29\n",
            "2024-06-26 12:33:01 (UTC) - 0:00:53 - train - INFO - step: 000002 - done (%): 2.0 - loss: 1.462 - lr: 1.8e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3423.8 - avg_words_per_second: 3308.5 - ETA: >2024-06-26 13:05:22\n",
            "2024-06-26 12:33:21 (UTC) - 0:01:12 - train - INFO - step: 000003 - done (%): 3.0 - loss: 1.459 - lr: 5.2e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3344.5 - avg_words_per_second: 3320.5 - ETA: >2024-06-26 13:05:15\n",
            "2024-06-26 12:33:40 (UTC) - 0:01:31 - train - INFO - step: 000004 - done (%): 4.0 - loss: 1.391 - lr: 8.6e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3420.5 - avg_words_per_second: 3344.9 - ETA: >2024-06-26 13:05:01\n",
            "2024-06-26 12:33:59 (UTC) - 0:01:51 - train - INFO - step: 000005 - done (%): 5.0 - loss: 1.333 - lr: 1.0e-04 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3362.0 - avg_words_per_second: 3348.3 - ETA: >2024-06-26 13:04:59\n",
            "2024-06-26 12:34:19 (UTC) - 0:02:11 - train - INFO - step: 000006 - done (%): 6.0 - loss: 1.347 - lr: 1.0e-04 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3308.8 - avg_words_per_second: 3341.7 - ETA: >2024-06-26 13:05:03\n",
            "2024-06-26 12:34:39 (UTC) - 0:02:30 - train - INFO - step: 000007 - done (%): 7.0 - loss: 1.368 - lr: 1.0e-04 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3331.9 - avg_words_per_second: 3340.3 - ETA: >2024-06-26 13:05:03\n",
            "2024-06-26 12:34:58 (UTC) - 0:02:50 - train - INFO - step: 000008 - done (%): 8.0 - loss: 1.387 - lr: 1.0e-04 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3389.2 - avg_words_per_second: 3346.3 - ETA: >2024-06-26 13:05:00\n",
            "2024-06-26 12:35:18 (UTC) - 0:03:09 - train - INFO - step: 000009 - done (%): 9.0 - loss: 1.342 - lr: 1.0e-04 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3371.1 - avg_words_per_second: 3349.0 - ETA: >2024-06-26 13:04:58\n",
            "2024-06-26 12:35:37 (UTC) - 0:03:28 - train - INFO - step: 000010 - done (%): 10.0 - loss: 1.395 - lr: 9.9e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3406.6 - avg_words_per_second: 3354.7 - ETA: >2024-06-26 13:04:55\n",
            "2024-06-26 12:35:56 (UTC) - 0:03:48 - train - INFO - step: 000011 - done (%): 11.0 - loss: 1.315 - lr: 9.9e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3365.3 - avg_words_per_second: 3355.7 - ETA: >2024-06-26 13:04:54\n",
            "2024-06-26 12:36:16 (UTC) - 0:04:07 - train - INFO - step: 000012 - done (%): 12.0 - loss: 1.356 - lr: 9.9e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3384.4 - avg_words_per_second: 3358.1 - ETA: >2024-06-26 13:04:53\n",
            "2024-06-26 12:36:35 (UTC) - 0:04:27 - train - INFO - step: 000013 - done (%): 13.0 - loss: 1.370 - lr: 9.8e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3314.1 - avg_words_per_second: 3354.6 - ETA: >2024-06-26 13:04:55\n",
            "2024-06-26 12:36:55 (UTC) - 0:04:46 - train - INFO - step: 000014 - done (%): 14.0 - loss: 1.330 - lr: 9.8e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3409.8 - avg_words_per_second: 3358.5 - ETA: >2024-06-26 13:04:53\n",
            "2024-06-26 12:37:14 (UTC) - 0:05:06 - train - INFO - step: 000015 - done (%): 15.0 - loss: 1.252 - lr: 9.7e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3374.9 - avg_words_per_second: 3359.6 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:37:34 (UTC) - 0:05:25 - train - INFO - step: 000016 - done (%): 16.0 - loss: 1.388 - lr: 9.7e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3341.8 - avg_words_per_second: 3358.5 - ETA: >2024-06-26 13:04:53\n",
            "2024-06-26 12:37:53 (UTC) - 0:05:45 - train - INFO - step: 000017 - done (%): 17.0 - loss: 1.354 - lr: 9.6e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3353.9 - avg_words_per_second: 3358.2 - ETA: >2024-06-26 13:04:53\n",
            "2024-06-26 12:38:13 (UTC) - 0:06:04 - train - INFO - step: 000018 - done (%): 18.0 - loss: 1.328 - lr: 9.5e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3357.0 - avg_words_per_second: 3358.1 - ETA: >2024-06-26 13:04:53\n",
            "2024-06-26 12:38:32 (UTC) - 0:06:24 - train - INFO - step: 000019 - done (%): 19.0 - loss: 1.350 - lr: 9.5e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3399.4 - avg_words_per_second: 3360.3 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:38:52 (UTC) - 0:06:43 - train - INFO - step: 000020 - done (%): 20.0 - loss: 1.336 - lr: 9.4e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3349.4 - avg_words_per_second: 3359.7 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:39:11 (UTC) - 0:07:03 - train - INFO - step: 000021 - done (%): 21.0 - loss: 1.337 - lr: 9.3e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3352.4 - avg_words_per_second: 3359.4 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:39:31 (UTC) - 0:07:22 - train - INFO - step: 000022 - done (%): 22.0 - loss: 1.383 - lr: 9.2e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3377.7 - avg_words_per_second: 3360.2 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:39:50 (UTC) - 0:07:42 - train - INFO - step: 000023 - done (%): 23.0 - loss: 1.353 - lr: 9.1e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3365.6 - avg_words_per_second: 3360.5 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:40:10 (UTC) - 0:08:01 - train - INFO - step: 000024 - done (%): 24.0 - loss: 1.377 - lr: 9.0e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3347.4 - avg_words_per_second: 3359.9 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:40:29 (UTC) - 0:08:20 - train - INFO - step: 000025 - done (%): 25.0 - loss: 1.337 - lr: 8.9e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3416.1 - avg_words_per_second: 3362.1 - ETA: >2024-06-26 13:04:51\n",
            "2024-06-26 12:40:48 (UTC) - 0:08:40 - train - INFO - step: 000026 - done (%): 26.0 - loss: 1.350 - lr: 8.8e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3326.2 - avg_words_per_second: 3360.7 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:41:08 (UTC) - 0:08:59 - train - INFO - step: 000027 - done (%): 27.0 - loss: 1.345 - lr: 8.7e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3379.3 - avg_words_per_second: 3361.4 - ETA: >2024-06-26 13:04:51\n",
            "2024-06-26 12:41:27 (UTC) - 0:09:19 - train - INFO - step: 000028 - done (%): 28.0 - loss: 1.405 - lr: 8.6e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3349.8 - avg_words_per_second: 3361.0 - ETA: >2024-06-26 13:04:51\n",
            "2024-06-26 12:41:47 (UTC) - 0:09:38 - train - INFO - step: 000029 - done (%): 29.0 - loss: 1.430 - lr: 8.5e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3384.5 - avg_words_per_second: 3361.8 - ETA: >2024-06-26 13:04:51\n",
            "2024-06-26 12:42:06 (UTC) - 0:09:58 - train - INFO - step: 000030 - done (%): 30.0 - loss: 1.335 - lr: 8.4e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3390.2 - avg_words_per_second: 3362.7 - ETA: >2024-06-26 13:04:50\n",
            "2024-06-26 12:42:26 (UTC) - 0:10:17 - train - INFO - step: 000031 - done (%): 31.0 - loss: 1.404 - lr: 8.3e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3356.9 - avg_words_per_second: 3362.6 - ETA: >2024-06-26 13:04:50\n",
            "2024-06-26 12:42:45 (UTC) - 0:10:37 - train - INFO - step: 000032 - done (%): 32.0 - loss: 1.278 - lr: 8.1e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3364.1 - avg_words_per_second: 3362.6 - ETA: >2024-06-26 13:04:50\n",
            "2024-06-26 12:43:05 (UTC) - 0:10:56 - train - INFO - step: 000033 - done (%): 33.0 - loss: 1.376 - lr: 8.0e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3361.8 - avg_words_per_second: 3362.6 - ETA: >2024-06-26 13:04:50\n",
            "2024-06-26 12:43:24 (UTC) - 0:11:16 - train - INFO - step: 000034 - done (%): 34.0 - loss: 1.394 - lr: 7.9e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3332.3 - avg_words_per_second: 3361.7 - ETA: >2024-06-26 13:04:51\n",
            "2024-06-26 12:43:44 (UTC) - 0:11:36 - train - INFO - step: 000035 - done (%): 35.0 - loss: 1.331 - lr: 7.7e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3312.6 - avg_words_per_second: 3360.3 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:44:04 (UTC) - 0:11:55 - train - INFO - step: 000036 - done (%): 36.0 - loss: 1.325 - lr: 7.6e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3353.4 - avg_words_per_second: 3360.1 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:44:23 (UTC) - 0:12:14 - train - INFO - step: 000037 - done (%): 37.0 - loss: 1.314 - lr: 7.5e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3395.4 - avg_words_per_second: 3361.0 - ETA: >2024-06-26 13:04:51\n",
            "2024-06-26 12:44:42 (UTC) - 0:12:34 - train - INFO - step: 000038 - done (%): 38.0 - loss: 1.356 - lr: 7.3e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3361.2 - avg_words_per_second: 3361.0 - ETA: >2024-06-26 13:04:51\n",
            "2024-06-26 12:45:03 (UTC) - 0:12:54 - train - INFO - step: 000039 - done (%): 39.0 - loss: 1.438 - lr: 7.2e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3267.3 - avg_words_per_second: 3358.5 - ETA: >2024-06-26 13:04:53\n",
            "2024-06-26 12:45:22 (UTC) - 0:13:13 - train - INFO - step: 000040 - done (%): 40.0 - loss: 1.352 - lr: 7.0e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3386.7 - avg_words_per_second: 3359.2 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:45:41 (UTC) - 0:13:33 - train - INFO - step: 000041 - done (%): 41.0 - loss: 1.281 - lr: 6.9e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3373.3 - avg_words_per_second: 3359.6 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:46:01 (UTC) - 0:13:52 - train - INFO - step: 000042 - done (%): 42.0 - loss: 1.277 - lr: 6.7e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3376.3 - avg_words_per_second: 3360.0 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:46:20 (UTC) - 0:14:12 - train - INFO - step: 000043 - done (%): 43.0 - loss: 1.325 - lr: 6.5e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3351.7 - avg_words_per_second: 3359.8 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:46:40 (UTC) - 0:14:31 - train - INFO - step: 000044 - done (%): 44.0 - loss: 1.349 - lr: 6.4e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3335.7 - avg_words_per_second: 3359.2 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:46:59 (UTC) - 0:14:51 - train - INFO - step: 000045 - done (%): 45.0 - loss: 1.378 - lr: 6.2e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3387.4 - avg_words_per_second: 3359.9 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:47:19 (UTC) - 0:15:10 - train - INFO - step: 000046 - done (%): 46.0 - loss: 1.375 - lr: 6.1e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3347.0 - avg_words_per_second: 3359.6 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:47:39 (UTC) - 0:15:30 - train - INFO - step: 000047 - done (%): 47.0 - loss: 1.326 - lr: 5.9e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3313.2 - avg_words_per_second: 3358.6 - ETA: >2024-06-26 13:04:53\n",
            "2024-06-26 12:47:58 (UTC) - 0:15:50 - train - INFO - step: 000048 - done (%): 48.0 - loss: 1.345 - lr: 5.7e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3362.8 - avg_words_per_second: 3358.7 - ETA: >2024-06-26 13:04:53\n",
            "2024-06-26 12:48:18 (UTC) - 0:16:09 - train - INFO - step: 000049 - done (%): 49.0 - loss: 1.333 - lr: 5.6e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3344.7 - avg_words_per_second: 3358.4 - ETA: >2024-06-26 13:04:53\n",
            "2024-06-26 12:48:37 (UTC) - 0:16:29 - train - INFO - step: 000050 - done (%): 50.0 - loss: 1.266 - lr: 5.4e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3375.2 - avg_words_per_second: 3358.7 - ETA: >2024-06-26 13:04:53\n",
            "2024-06-26 12:48:56 (UTC) - 0:16:48 - train - INFO - step: 000051 - done (%): 51.0 - loss: 1.350 - lr: 5.2e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3401.9 - avg_words_per_second: 3359.5 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:49:16 (UTC) - 0:17:07 - train - INFO - step: 000052 - done (%): 52.0 - loss: 1.296 - lr: 5.1e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3357.8 - avg_words_per_second: 3359.5 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:49:35 (UTC) - 0:17:27 - train - INFO - step: 000053 - done (%): 53.0 - loss: 1.352 - lr: 4.9e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3354.2 - avg_words_per_second: 3359.4 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:49:55 (UTC) - 0:17:47 - train - INFO - step: 000054 - done (%): 54.0 - loss: 1.365 - lr: 4.8e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3334.6 - avg_words_per_second: 3359.0 - ETA: >2024-06-26 13:04:53\n",
            "2024-06-26 12:50:15 (UTC) - 0:18:06 - train - INFO - step: 000055 - done (%): 55.0 - loss: 1.322 - lr: 4.6e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3373.8 - avg_words_per_second: 3359.2 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:50:34 (UTC) - 0:18:26 - train - INFO - step: 000056 - done (%): 56.0 - loss: 1.324 - lr: 4.4e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3356.7 - avg_words_per_second: 3359.2 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:50:53 (UTC) - 0:18:45 - train - INFO - step: 000057 - done (%): 57.0 - loss: 1.386 - lr: 4.3e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3379.6 - avg_words_per_second: 3359.5 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:51:13 (UTC) - 0:19:04 - train - INFO - step: 000058 - done (%): 58.0 - loss: 1.358 - lr: 4.1e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3386.0 - avg_words_per_second: 3360.0 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:51:32 (UTC) - 0:19:24 - train - INFO - step: 000059 - done (%): 59.0 - loss: 1.298 - lr: 3.9e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3364.6 - avg_words_per_second: 3360.1 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:51:52 (UTC) - 0:19:44 - train - INFO - step: 000060 - done (%): 60.0 - loss: 1.416 - lr: 3.8e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3319.3 - avg_words_per_second: 3359.4 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:52:11 (UTC) - 0:20:03 - train - INFO - step: 000061 - done (%): 61.0 - loss: 1.352 - lr: 3.6e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3393.1 - avg_words_per_second: 3359.9 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:52:31 (UTC) - 0:20:22 - train - INFO - step: 000062 - done (%): 62.0 - loss: 1.323 - lr: 3.5e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3338.0 - avg_words_per_second: 3359.6 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:52:50 (UTC) - 0:20:42 - train - INFO - step: 000063 - done (%): 63.0 - loss: 1.304 - lr: 3.3e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3362.7 - avg_words_per_second: 3359.6 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:53:10 (UTC) - 0:21:01 - train - INFO - step: 000064 - done (%): 64.0 - loss: 1.338 - lr: 3.1e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3362.1 - avg_words_per_second: 3359.7 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:53:30 (UTC) - 0:21:21 - train - INFO - step: 000065 - done (%): 65.0 - loss: 1.367 - lr: 3.0e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3304.8 - avg_words_per_second: 3358.8 - ETA: >2024-06-26 13:04:53\n",
            "2024-06-26 12:53:49 (UTC) - 0:21:41 - train - INFO - step: 000066 - done (%): 66.0 - loss: 1.319 - lr: 2.8e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3355.2 - avg_words_per_second: 3358.7 - ETA: >2024-06-26 13:04:53\n",
            "2024-06-26 12:54:09 (UTC) - 0:22:00 - train - INFO - step: 000067 - done (%): 67.0 - loss: 1.311 - lr: 2.7e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3417.3 - avg_words_per_second: 3359.6 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:54:28 (UTC) - 0:22:19 - train - INFO - step: 000068 - done (%): 68.0 - loss: 1.355 - lr: 2.5e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3379.6 - avg_words_per_second: 3359.9 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:54:47 (UTC) - 0:22:39 - train - INFO - step: 000069 - done (%): 69.0 - loss: 1.385 - lr: 2.4e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3383.5 - avg_words_per_second: 3360.2 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:55:07 (UTC) - 0:22:58 - train - INFO - step: 000070 - done (%): 70.0 - loss: 1.378 - lr: 2.3e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3334.5 - avg_words_per_second: 3359.9 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:55:26 (UTC) - 0:23:18 - train - INFO - step: 000071 - done (%): 71.0 - loss: 1.385 - lr: 2.1e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3350.9 - avg_words_per_second: 3359.7 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:55:46 (UTC) - 0:23:37 - train - INFO - step: 000072 - done (%): 72.0 - loss: 1.392 - lr: 2.0e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3369.2 - avg_words_per_second: 3359.9 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:56:05 (UTC) - 0:23:57 - train - INFO - step: 000073 - done (%): 73.0 - loss: 1.408 - lr: 1.9e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3414.5 - avg_words_per_second: 3360.6 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:56:25 (UTC) - 0:24:16 - train - INFO - step: 000074 - done (%): 74.0 - loss: 1.360 - lr: 1.7e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3308.0 - avg_words_per_second: 3359.9 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:56:44 (UTC) - 0:24:36 - train - INFO - step: 000075 - done (%): 75.0 - loss: 1.305 - lr: 1.6e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3383.7 - avg_words_per_second: 3360.2 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:57:04 (UTC) - 0:24:56 - train - INFO - step: 000076 - done (%): 76.0 - loss: 1.342 - lr: 1.5e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3289.0 - avg_words_per_second: 3359.2 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:57:24 (UTC) - 0:25:16 - train - INFO - step: 000077 - done (%): 77.0 - loss: 1.283 - lr: 1.4e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3313.8 - avg_words_per_second: 3358.6 - ETA: >2024-06-26 13:04:53\n",
            "2024-06-26 12:57:43 (UTC) - 0:25:35 - train - INFO - step: 000078 - done (%): 78.0 - loss: 1.250 - lr: 1.3e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3371.1 - avg_words_per_second: 3358.8 - ETA: >2024-06-26 13:04:53\n",
            "2024-06-26 12:58:03 (UTC) - 0:25:54 - train - INFO - step: 000079 - done (%): 79.0 - loss: 1.471 - lr: 1.2e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3392.4 - avg_words_per_second: 3359.2 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:58:22 (UTC) - 0:26:14 - train - INFO - step: 000080 - done (%): 80.0 - loss: 1.315 - lr: 1.1e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3350.9 - avg_words_per_second: 3359.1 - ETA: >2024-06-26 13:04:53\n",
            "2024-06-26 12:58:42 (UTC) - 0:26:33 - train - INFO - step: 000081 - done (%): 81.0 - loss: 1.332 - lr: 9.5e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3406.2 - avg_words_per_second: 3359.7 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:59:01 (UTC) - 0:26:53 - train - INFO - step: 000082 - done (%): 82.0 - loss: 1.325 - lr: 8.6e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3355.4 - avg_words_per_second: 3359.6 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:59:21 (UTC) - 0:27:12 - train - INFO - step: 000083 - done (%): 83.0 - loss: 1.329 - lr: 7.7e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3335.9 - avg_words_per_second: 3359.4 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 12:59:40 (UTC) - 0:27:32 - train - INFO - step: 000084 - done (%): 84.0 - loss: 1.346 - lr: 6.8e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3358.5 - avg_words_per_second: 3359.3 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 13:00:00 (UTC) - 0:27:51 - train - INFO - step: 000085 - done (%): 85.0 - loss: 1.345 - lr: 6.0e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3391.4 - avg_words_per_second: 3359.7 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 13:00:19 (UTC) - 0:28:11 - train - INFO - step: 000086 - done (%): 86.0 - loss: 1.291 - lr: 5.3e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3373.9 - avg_words_per_second: 3359.9 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 13:00:38 (UTC) - 0:28:30 - train - INFO - step: 000087 - done (%): 87.0 - loss: 1.274 - lr: 4.6e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3437.6 - avg_words_per_second: 3360.8 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 13:00:58 (UTC) - 0:28:49 - train - INFO - step: 000088 - done (%): 88.0 - loss: 1.242 - lr: 3.9e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3370.6 - avg_words_per_second: 3360.9 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 13:01:17 (UTC) - 0:29:09 - train - INFO - step: 000089 - done (%): 89.0 - loss: 1.282 - lr: 3.3e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3357.0 - avg_words_per_second: 3360.8 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 13:01:36 (UTC) - 0:29:28 - train - INFO - step: 000090 - done (%): 90.0 - loss: 1.317 - lr: 2.7e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3380.3 - avg_words_per_second: 3361.0 - ETA: >2024-06-26 13:04:51\n",
            "2024-06-26 13:01:56 (UTC) - 0:29:48 - train - INFO - step: 000091 - done (%): 91.0 - loss: 1.346 - lr: 2.2e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3341.4 - avg_words_per_second: 3360.8 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 13:02:16 (UTC) - 0:30:07 - train - INFO - step: 000092 - done (%): 92.0 - loss: 1.346 - lr: 1.7e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3370.3 - avg_words_per_second: 3360.9 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 13:02:35 (UTC) - 0:30:27 - train - INFO - step: 000093 - done (%): 93.0 - loss: 1.338 - lr: 1.3e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3302.9 - avg_words_per_second: 3360.3 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 13:02:55 (UTC) - 0:30:46 - train - INFO - step: 000094 - done (%): 94.0 - loss: 1.302 - lr: 9.8e-07 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3394.4 - avg_words_per_second: 3360.6 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 13:03:14 (UTC) - 0:31:05 - train - INFO - step: 000095 - done (%): 95.0 - loss: 1.398 - lr: 6.8e-07 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3409.3 - avg_words_per_second: 3361.2 - ETA: >2024-06-26 13:04:51\n",
            "2024-06-26 13:03:33 (UTC) - 0:31:25 - train - INFO - step: 000096 - done (%): 96.0 - loss: 1.278 - lr: 4.4e-07 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3364.5 - avg_words_per_second: 3361.2 - ETA: >2024-06-26 13:04:51\n",
            "2024-06-26 13:03:53 (UTC) - 0:31:45 - train - INFO - step: 000097 - done (%): 97.0 - loss: 1.286 - lr: 2.5e-07 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3328.7 - avg_words_per_second: 3360.8 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 13:04:13 (UTC) - 0:32:04 - train - INFO - step: 000098 - done (%): 98.0 - loss: 1.187 - lr: 1.1e-07 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3373.5 - avg_words_per_second: 3361.0 - ETA: >2024-06-26 13:04:51\n",
            "2024-06-26 13:04:32 (UTC) - 0:32:23 - train - INFO - step: 000099 - done (%): 99.0 - loss: 1.329 - lr: 2.8e-08 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3364.6 - avg_words_per_second: 3361.0 - ETA: >2024-06-26 13:04:51\n",
            "2024-06-26 13:04:52 (UTC) - 0:32:43 - train - INFO - step: 000100 - done (%): 100.0 - loss: 1.279 - lr: 4.0e-10 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3352.2 - avg_words_per_second: 3360.9 - ETA: >2024-06-26 13:04:52\n",
            "2024-06-26 13:04:52 (UTC) - 0:32:43 - checkpointing - INFO - Dumping checkpoint in /content/governmentgpt/checkpoints/checkpoint_000100/consolidated using tmp name: tmp.consolidated\n",
            "2024-06-26 13:04:52 (UTC) - 0:32:44 - checkpointing - INFO - Done dumping checkpoint in /content/governmentgpt/checkpoints/checkpoint_000100/consolidated for step: 100\n",
            "2024-06-26 13:04:52 (UTC) - 0:32:44 - checkpointing - INFO - Done deleting checkpoints \n",
            "2024-06-26 13:04:52 (UTC) - 0:32:44 - checkpointing - INFO - Done!\n",
            "2024-06-26 13:04:52 (UTC) - 0:32:44 - train - INFO - done!\n",
            "2024-06-26 13:04:52 (UTC) - 0:32:44 - utils - INFO - Closing: eval_logger\n",
            "2024-06-26 13:04:52 (UTC) - 0:32:44 - utils - INFO - Closed: eval_logger\n",
            "2024-06-26 13:04:52 (UTC) - 0:32:44 - utils - INFO - Closing: metrics_logger\n",
            "2024-06-26 13:04:52 (UTC) - 0:32:44 - utils - INFO - Closed: metrics_logger\n",
            "2024-06-26 13:04:52 (UTC) - 0:32:44 - train - INFO - Closed everything!\n"
          ]
        }
      ],
      "source": [
        "# start training\n",
        "%cd /content/mistral-finetune/\n",
        "!torchrun --nproc-per-node 1 -m train /content/config/governmentgpt.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruJ29JFn98zE"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BWNGKt9-Kxz",
        "outputId": "771effa4-1df5-49ba-bba0-d3c6f46484b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mistral_inference\n",
            "  Downloading mistral_inference-1.1.0-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: fire>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from mistral_inference) (0.6.0)\n",
            "Requirement already satisfied: mistral_common<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from mistral_inference) (1.2.1)\n",
            "Requirement already satisfied: safetensors>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mistral_inference) (0.4.3)\n",
            "Requirement already satisfied: simple-parsing>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from mistral_inference) (0.1.5)\n",
            "Requirement already satisfied: xformers>=0.0.24 in /usr/local/lib/python3.10/dist-packages (from mistral_inference) (0.0.24)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire>=0.6.0->mistral_inference) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire>=0.6.0->mistral_inference) (2.4.0)\n",
            "Requirement already satisfied: jsonschema==4.21.1 in /usr/local/lib/python3.10/dist-packages (from mistral_common<2.0.0,>=1.0.0->mistral_inference) (4.21.1)\n",
            "Requirement already satisfied: pydantic==2.6.1 in /usr/local/lib/python3.10/dist-packages (from mistral_common<2.0.0,>=1.0.0->mistral_inference) (2.6.1)\n",
            "Requirement already satisfied: sentencepiece==0.1.99 in /usr/local/lib/python3.10/dist-packages (from mistral_common<2.0.0,>=1.0.0->mistral_inference) (0.1.99)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.10/dist-packages (from mistral_common<2.0.0,>=1.0.0->mistral_inference) (4.12.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.21.1->mistral_common<2.0.0,>=1.0.0->mistral_inference) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.21.1->mistral_common<2.0.0,>=1.0.0->mistral_inference) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.21.1->mistral_common<2.0.0,>=1.0.0->mistral_inference) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.21.1->mistral_common<2.0.0,>=1.0.0->mistral_inference) (0.18.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic==2.6.1->mistral_common<2.0.0,>=1.0.0->mistral_inference) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic==2.6.1->mistral_common<2.0.0,>=1.0.0->mistral_inference) (2.16.2)\n",
            "Requirement already satisfied: docstring-parser~=0.15 in /usr/local/lib/python3.10/dist-packages (from simple-parsing>=0.1.5->mistral_inference) (0.16)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers>=0.0.24->mistral_inference) (1.25.2)\n",
            "Requirement already satisfied: torch==2.2.0 in /usr/local/lib/python3.10/dist-packages (from xformers>=0.0.24->mistral_inference) (2.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers>=0.0.24->mistral_inference) (3.15.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers>=0.0.24->mistral_inference) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers>=0.0.24->mistral_inference) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers>=0.0.24->mistral_inference) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers>=0.0.24->mistral_inference) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers>=0.0.24->mistral_inference) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers>=0.0.24->mistral_inference) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers>=0.0.24->mistral_inference) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers>=0.0.24->mistral_inference) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers>=0.0.24->mistral_inference) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers>=0.0.24->mistral_inference) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers>=0.0.24->mistral_inference) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers>=0.0.24->mistral_inference) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers>=0.0.24->mistral_inference) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers>=0.0.24->mistral_inference) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers>=0.0.24->mistral_inference) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers>=0.0.24->mistral_inference) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0->xformers>=0.0.24->mistral_inference) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.0->xformers>=0.0.24->mistral_inference) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.0->xformers>=0.0.24->mistral_inference) (1.3.0)\n",
            "Installing collected packages: mistral_inference\n",
            "Successfully installed mistral_inference-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install mistral_inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIkpJjI25Gal",
        "outputId": "2b46bea4-e140-4694-d670-86faf1edc0ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jun 26 13:05:01 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P0              51W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "F-xLs2Ot9-il"
      },
      "outputs": [],
      "source": [
        "from mistral_inference.model import Transformer\n",
        "from mistral_inference.generate import generate\n",
        "\n",
        "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
        "from mistral_common.protocol.instruct.messages import UserMessage\n",
        "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
        "\n",
        "tokenizer = MistralTokenizer.from_file(\"/content/mistral_models/7B-v0.3/tokenizer.model.v3\")  # change to extracted tokenizer file\n",
        "\n",
        "# Clear GPU memory first\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "model = Transformer.from_folder(\"/content/mistral_models/7B-v0.3\")  # change to extracted model dir\n",
        "model.load_lora(\"/content/governmentgpt/checkpoints/checkpoint_000100/consolidated/lora.safetensors\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vd8A8JP4Fx3C",
        "outputId": "14f26993-305e-4950-b7cf-a6155e454b67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker: Conservative MP for Worthing West (additional roles: Member of Scottish Affairs Committee.): \n",
            "\n",
            " Speech transcript: A number of measures have been put in place, and the Government will make an announcement later today, and I look forward to the opportunity of having discussions with my honorable friend on this issue. Speaker: Conservative MP for East Worthing and Shoreham (additional roles: Member of Levelling Up Housing And Communities Committee.): \n",
            "\n",
            " Speech transcript: What discussions he has had with the Secretary of State for Health and Social Care on promoting innovation in mental health services. Speaker: Conservative MP for Worthing West (additional roles: Member of Scottish Affairs Committee.): \n",
            "\n",
            " Speech transcript: Mental health has been an issue I have focused on since I became an MP, so I am particularly pleased that my department is leading the way in driving forward new innovative mental health services in this country. We are pleased to have taken the advice of the Secretary of State for Health and Social Care in the recent announcements on mental health services. Speaker: Conservative MP for East Worthing and Shoreham (additional roles: Member of Levelling Up Housing And Communities Committee.): \n",
            "\n",
            " Speech transcript: I thank my hon. Friend for his continued commitment to promoting innovation in mental health services. Does he agree that greater collaboration between universities and the NHS will help to ensure that innovation is spread right across the NHS and that patients benefit from this? Speaker: Conservative MP for Worthing West (additional roles: Member of Scottish Affairs Committee.): \n",
            "\n",
            " Speech transcript: I agree with my hon. Friend. Universities play a crucial role in the development of innovative mental health services, and they have been involved in a number of initiatives that the Government have funded. The university mental health crisis service has helped the NHS to develop crisis services in many parts of the country. The Government are continuing to invest in such innovative services, and I am sure that universities will continue to play an important role. Speaker: Conservative MP for Harlow (additional roles: Member of Education Committee.): \n",
            "\n",
            " Speech transcript: What discussions he has had with the Secretary of State for Health and Social Care on supporting the NHS to improve the treatment of mental health. Speaker: Conservative MP for Worthing West (additional roles: Member of Scottish Affairs Committee.): \n",
            "\n",
            " Speech transcript: I have regular meetings with the Secretary of State, as have my officials. The Government are determined to improve mental health services, which is why we have announced a record investment of more than £1.2 billion in 2019-20. That will allow the NHS to employ an additional 21,000 mental health workers, which is a significant step forward. Speaker: Conservative MP for Harlow (additional roles: Member of Education Committee.): \n",
            "\n",
            " Speech transcript: I thank my hon. Friend for that answer. What steps are being taken to increase the number of NHS staff to meet the demands of the extra mental health services that are being commissioned? Speaker: Conservative MP for Worthing West (additional roles: Member of Scottish Affairs Committee.): \n",
            "\n",
            " Speech transcript: We have taken a number of steps to improve the capacity of the NHS to deliver mental health services, and my hon. Friend will be aware that an additional £1.2 billion is being invested in mental health services this year. That will allow the NHS to employ 21,000 additional mental health workers, and it is already rolling out new mental health services, such as crisis teams, which are already being introduced across the country. Speaker: Labour MP for Luton South (additional roles: House Of Commons Shadow Minister (Health and Social Care).): \n",
            "\n",
            " Speech transcript: The Minister is quite right to praise universities for their role in research, but there is a risk that we will miss out on important research if we do not support our mental health services properly. Will he take on board the Mental Health Policy Groupâs warning that we need 25,000 extra mental health nurses and more therapists to support them? Speaker: Conservative MP for Worthing West (additional roles: Member of Scottish Affairs Committee.): \n",
            "\n",
            " Speech transcript: We are committed to making sure that the NHS has the staff it needs, and we are investing in mental health services. As I said earlier, we are investing an additional £1.2 billion in mental health services this year, which will allow us to employ an additional 21,000 mental health workers. Speaker: Conservative MP for Amber Valley (additional roles: Member of Panel Of Chairs.): \n",
            "\n",
            " Speech transcript: What recent assessment he has made of the potential merits of the application of artificial intelligence to support medical research and clinical practice. Speaker: Conservative MP for Bromsgrove: \n",
            "\n",
            " Speech transcript: The Government have been investing in artificial intelligence in the health and life sciences sector since 2014, and the Minister for Life Sciences, my hon. Friend , has announced a new artificial intelligence health and care award to support AI innovators in the NHS. Speaker: Conservative MP for Amber Valley (additional roles: Member of Panel Of Chairs.): \n",
            "\n",
            " Speech transcript: I thank the Minister for that answer. The UK is a global leader in life sciences, but the application of AI in the health sector is still in its infancy. Does the Minister agree that we should strive to build on the UKâs position as a world leader in the application of AI to support medical research and clinical practice? Speaker: Conservative MP for Bromsgrove: \n",
            "\n",
            " Speech transcript: Yes, and we are already making a good start. Last year, the Department for Business, Energy and Industrial Strategy invested Â£10 million in AI health and care awards, and this year my right hon. Friend the Minister for Life Sciences has committed an additional Â£10 million to the programme. Speaker: Labour MP for Kingston upon Hull North (additional roles: Member of Procedure Committee, House Of Commons Shadow Minister (Home Office).): \n",
            "\n",
            " Speech transcript: AI will be critical to the development of new medicines. As a member of the all-party parliamentary group on the application of AI in the life sciences, may I invite the Minister to join me and the group in meeting representatives of the industry in the near future? Speaker: Conservative MP for Bromsgrove: \n",
            "\n",
            " Speech transcript: The hon. Lady makes a good point. I think we will be able to join her in the near future to talk about the important role that AI plays in research and clinical practice. Speaker: Conservative MP for Bury North (additional roles: Member of Transport Committee.): \n",
            "\n",
            " Speech transcript: What recent assessment he has made of the potential merits of the application of artificial intelligence to support medical research and clinical practice. Speaker: Conservative MP for Bromsgrove: \n",
            "\n",
            " Speech transcript: The Government have been investing in artificial intelligence in the health and life sciences sector since 2014, and the Minister for Life Sciences, my hon. Friend , has announced a new artificial intelligence health and care award to support AI innovators in the NHS. Speaker: Conservative MP for Bury North (additional roles: Member of Transport Committee.): \n",
            "\n",
            " Speech transcript: I thank the Minister for that answer. AI has huge potential to transform medical research and clinical practice, and could save millions of lives. Can he tell me what steps the Government are taking to ensure that the UK leads the way in developing AI? Speaker: Conservative MP for Bromsgrove: \n",
            "\n",
            " Speech transcript: Yes, we are investing in AI in the health and life sciences sector. Last year, the Department for Business, Energy and Industrial Strategy invested Â£10 million in AI health and care awards, and my right hon. Friend the Minister for Life Sciences has committed an additional Â£10 million to the programme. Speaker: Conservative MP for Stafford (additional roles: Member of Panel Of Chairs.): \n",
            "\n",
            " Speech transcript: What recent assessment he has made of trends in the availability of medicines. Speaker: Conservative MP for Bury St Edmunds (additional roles: House Of Commons The Parliamentary Under-Secretary for Health and Social Care.): \n",
            "\n",
            " Speech transcript: The Government take the availability of medicines seriously. We regularly assess the state of the market and work with manufacturers to improve the supply of medicines. We have a comprehensive package of measures in place to ensure that we are prepared for all eventualities, including leaving the European Union with no deal. Speaker: Conservative MP for Stafford (additional roles: Member of Panel Of Chairs.): \n",
            "\n",
            " Speech transcript: I am sure that my hon. Friend is aware of the shortage of antipsychotic medicines for patients with dementia, and of the impact on patients. What measures are being taken to make sure that there are no such shortages in the future? Speaker: Conservative MP for Bury St Edmunds (additional roles: House Of Commons The Parliamentary Under-Secretary for Health and Social Care.): \n",
            "\n",
            " Speech transcript: I thank my hon. Friend for raising this issue. I am aware of it and the concerns of stakeholders, and we have a plan to\n"
          ]
        }
      ],
      "source": [
        "content_1 = \"Speaker: Labour MP for Durham: Speech transcript: I am deeply concerned about the risk to a dwindling supply of rich tea biscuits that is being reported by the press due to biscuit factory worker strikes. As righteous British citizens we must protect our most important National biscuit identity for our tea breaks. Can the honorable gentleman outline what they intend to do about it?\"\n",
        "content_2 = \"Speaker: Conservative MP for Norwich: \\n\\n Speech transcript: What plans have we to support the biscuit manufacturing industry in the north east?\"\n",
        "content_3 = \"Speaker: Labour MP for Ipswich: \\n\\n Speech transcript: It is clear the finances of this country are in a dire state following their 7 years of power. We need fresh thinking to address the systemic issues. What policies does the Tory government plan to introduce?\"\n",
        "\n",
        "content_4 = \"Speaker: Liberal Democrat MP for Northwich: \\n\\n Speech transcript: Prolonged war at this point seems inevitable in Ukraine. We are in support of supply weapons for the long term, however we do not agree that we should make endless payments without strong agreeement as to what that money is intended for.\"\n",
        "\n",
        "\n",
        "content_6 = \"Speaker: Labour Democrat MP for Northwich: \\n\\n Speech transcript: The medical device industry in the UK seems to be in complete disarray, not least due to the uncertain regulatory environment imposed by Brexit. Can the honorable gentleman suggest what he will do to address this issue?\"\n",
        "\n",
        "\n",
        "\n",
        "content_5 = \"Speaker: Liberal Democrat MP for Northwich: \\n\\n Speech transcript: My consituents are writing to me voicing concern of the use of AI to replace their jobs. I share these concerns, not least because I think our role as MPs can be automated by using AI LLMs to debate on our behalf using our known political leanings. \"\n",
        "content_5 = content_5 + \"If this is the future - where do my honorable colleagues thinks will happen to our role as MPs in the future?\"\n",
        "\n",
        "\n",
        "completion_request = ChatCompletionRequest(messages=[UserMessage(content=content_6)])\n",
        "\n",
        "tokens = tokenizer.encode_chat_completion(completion_request).tokens\n",
        "\n",
        "out_tokens, _ = generate([tokens], model, max_tokens=2048, temperature=1.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id) # Set temperature to 1 for some creative dialogue\n",
        "result = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "38Ne69X6d3i7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3891025-c709-494b-dd60-f5e64c53a6d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Speaker: Conservative MP for Worthing West (additional roles: Member of Scottish Affairs Committee.):  \n",
            "Speech transcript: A number of measures have been put in place, and the Government will make an announcement later today, and I look forward to the opportunity of having discussions with my honorable friend on this issue. \n",
            "\n",
            "Speaker: Conservative MP for East Worthing and Shoreham (additional roles: Member of Levelling Up Housing And Communities Committee.):  \n",
            "Speech transcript: What discussions he has had with the Secretary of State for Health and Social Care on promoting innovation in mental health services. \n",
            "\n",
            "Speaker: Conservative MP for Worthing West (additional roles: Member of Scottish Affairs Committee.):  \n",
            "Speech transcript: Mental health has been an issue I have focused on since I became an MP, so I am particularly pleased that my department is leading the way in driving forward new innovative mental health services in this country. We are pleased to have taken the advice of the Secretary of State for Health and Social Care in the recent announcements on mental health services. \n",
            "\n",
            "Speaker: Conservative MP for East Worthing and Shoreham (additional roles: Member of Levelling Up Housing And Communities Committee.):  \n",
            "Speech transcript: I thank my hon. Friend for his continued commitment to promoting innovation in mental health services. Does he agree that greater collaboration between universities and the NHS will help to ensure that innovation is spread right across the NHS and that patients benefit from this? \n",
            "\n",
            "Speaker: Conservative MP for Worthing West (additional roles: Member of Scottish Affairs Committee.):  \n",
            "Speech transcript: I agree with my hon. Friend. Universities play a crucial role in the development of innovative mental health services, and they have been involved in a number of initiatives that the Government have funded. The university mental health crisis service has helped the NHS to develop crisis services in many parts of the country. The Government are continuing to invest in such innovative services, and I am sure that universities will continue to play an important role. \n",
            "\n",
            "Speaker: Conservative MP for Harlow (additional roles: Member of Education Committee.):  \n",
            "Speech transcript: What discussions he has had with the Secretary of State for Health and Social Care on supporting the NHS to improve the treatment of mental health. \n",
            "\n",
            "Speaker: Conservative MP for Worthing West (additional roles: Member of Scottish Affairs Committee.):  \n",
            "Speech transcript: I have regular meetings with the Secretary of State, as have my officials. The Government are determined to improve mental health services, which is why we have announced a record investment of more than £1.2 billion in 2019-20. That will allow the NHS to employ an additional 21,000 mental health workers, which is a significant step forward. \n",
            "\n",
            "Speaker: Conservative MP for Harlow (additional roles: Member of Education Committee.):  \n",
            "Speech transcript: I thank my hon. Friend for that answer. What steps are being taken to increase the number of NHS staff to meet the demands of the extra mental health services that are being commissioned? \n",
            "\n",
            "Speaker: Conservative MP for Worthing West (additional roles: Member of Scottish Affairs Committee.):  \n",
            "Speech transcript: We have taken a number of steps to improve the capacity of the NHS to deliver mental health services, and my hon. Friend will be aware that an additional £1.2 billion is being invested in mental health services this year. That will allow the NHS to employ 21,000 additional mental health workers, and it is already rolling out new mental health services, such as crisis teams, which are already being introduced across the country. \n",
            "\n",
            "Speaker: Labour MP for Luton South (additional roles: House Of Commons Shadow Minister (Health and Social Care).):  \n",
            "Speech transcript: The Minister is quite right to praise universities for their role in research, but there is a risk that we will miss out on important research if we do not support our mental health services properly. Will he take on board the Mental Health Policy Groupâs warning that we need 25,000 extra mental health nurses and more therapists to support them? \n",
            "\n",
            "Speaker: Conservative MP for Worthing West (additional roles: Member of Scottish Affairs Committee.):  \n",
            "Speech transcript: We are committed to making sure that the NHS has the staff it needs, and we are investing in mental health services. As I said earlier, we are investing an additional £1.2 billion in mental health services this year, which will allow us to employ an additional 21,000 mental health workers. \n",
            "\n",
            "Speaker: Conservative MP for Amber Valley (additional roles: Member of Panel Of Chairs.):  \n",
            "Speech transcript: What recent assessment he has made of the potential merits of the application of artificial intelligence to support medical research and clinical practice. \n",
            "\n",
            "Speaker: Conservative MP for Bromsgrove:  \n",
            "Speech transcript: The Government have been investing in artificial intelligence in the health and life sciences sector since 2014, and the Minister for Life Sciences, my hon. Friend , has announced a new artificial intelligence health and care award to support AI innovators in the NHS. \n",
            "\n",
            "Speaker: Conservative MP for Amber Valley (additional roles: Member of Panel Of Chairs.):  \n",
            "Speech transcript: I thank the Minister for that answer. The UK is a global leader in life sciences, but the application of AI in the health sector is still in its infancy. Does the Minister agree that we should strive to build on the UKâs position as a world leader in the application of AI to support medical research and clinical practice? \n",
            "\n",
            "Speaker: Conservative MP for Bromsgrove:  \n",
            "Speech transcript: Yes, and we are already making a good start. Last year, the Department for Business, Energy and Industrial Strategy invested Â£10 million in AI health and care awards, and this year my right hon. Friend the Minister for Life Sciences has committed an additional Â£10 million to the programme. \n",
            "\n",
            "Speaker: Labour MP for Kingston upon Hull North (additional roles: Member of Procedure Committee, House Of Commons Shadow Minister (Home Office).):  \n",
            "Speech transcript: AI will be critical to the development of new medicines. As a member of the all-party parliamentary group on the application of AI in the life sciences, may I invite the Minister to join me and the group in meeting representatives of the industry in the near future? \n",
            "\n",
            "Speaker: Conservative MP for Bromsgrove:  \n",
            "Speech transcript: The hon. Lady makes a good point. I think we will be able to join her in the near future to talk about the important role that AI plays in research and clinical practice. \n",
            "\n",
            "Speaker: Conservative MP for Bury North (additional roles: Member of Transport Committee.):  \n",
            "Speech transcript: What recent assessment he has made of the potential merits of the application of artificial intelligence to support medical research and clinical practice. \n",
            "\n",
            "Speaker: Conservative MP for Bromsgrove:  \n",
            "Speech transcript: The Government have been investing in artificial intelligence in the health and life sciences sector since 2014, and the Minister for Life Sciences, my hon. Friend , has announced a new artificial intelligence health and care award to support AI innovators in the NHS. \n",
            "\n",
            "Speaker: Conservative MP for Bury North (additional roles: Member of Transport Committee.):  \n",
            "Speech transcript: I thank the Minister for that answer. AI has huge potential to transform medical research and clinical practice, and could save millions of lives. Can he tell me what steps the Government are taking to ensure that the UK leads the way in developing AI? \n",
            "\n",
            "Speaker: Conservative MP for Bromsgrove:  \n",
            "Speech transcript: Yes, we are investing in AI in the health and life sciences sector. Last year, the Department for Business, Energy and Industrial Strategy invested Â£10 million in AI health and care awards, and my right hon. Friend the Minister for Life Sciences has committed an additional Â£10 million to the programme. \n",
            "\n",
            "Speaker: Conservative MP for Stafford (additional roles: Member of Panel Of Chairs.):  \n",
            "Speech transcript: What recent assessment he has made of trends in the availability of medicines. \n",
            "\n",
            "Speaker: Conservative MP for Bury St Edmunds (additional roles: House Of Commons The Parliamentary Under-Secretary for Health and Social Care.):  \n",
            "Speech transcript: The Government take the availability of medicines seriously. We regularly assess the state of the market and work with manufacturers to improve the supply of medicines. We have a comprehensive package of measures in place to ensure that we are prepared for all eventualities, including leaving the European Union with no deal. \n",
            "\n",
            "Speaker: Conservative MP for Stafford (additional roles: Member of Panel Of Chairs.):  \n",
            "Speech transcript: I am sure that my hon. Friend is aware of the shortage of antipsychotic medicines for patients with dementia, and of the impact on patients. What measures are being taken to make sure that there are no such shortages in the future? \n",
            "\n",
            "Speaker: Conservative MP for Bury St Edmunds (additional roles: House Of Commons The Parliamentary Under-Secretary for Health and Social Care.):  \n",
            "Speech transcript: I thank my hon. Friend for raising this issue. I am aware of it and the concerns of stakeholders, and we have a plan to\n"
          ]
        }
      ],
      "source": [
        "def format_output(text):\n",
        "  text = text.replace('\\n', '')\n",
        "  text = text.replace('Speaker:', '\\n\\nSpeaker:')\n",
        "  text = text.replace('Speech transcript:', '\\nSpeech transcript:')\n",
        "  return text\n",
        "\n",
        "print(format_output(result))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/governmentgpt.zip /content/governmentgpt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCClEuP63pOG",
        "outputId": "bc50ce52-48d3-4f6b-acd2-9078e52a4547"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/governmentgpt/ (stored 0%)\n",
            "  adding: content/governmentgpt/args.yaml (deflated 45%)\n",
            "  adding: content/governmentgpt/checkpoints/ (stored 0%)\n",
            "  adding: content/governmentgpt/checkpoints/checkpoint_000100/ (stored 0%)\n",
            "  adding: content/governmentgpt/checkpoints/checkpoint_000100/consolidated/ (stored 0%)\n",
            "  adding: content/governmentgpt/checkpoints/checkpoint_000100/consolidated/params.json (deflated 49%)\n",
            "  adding: content/governmentgpt/checkpoints/checkpoint_000100/consolidated/lora.safetensors (deflated 21%)\n",
            "  adding: content/governmentgpt/checkpoints/checkpoint_000100/consolidated/tokenizer.model.v3 (deflated 61%)\n",
            "  adding: content/governmentgpt/metrics.train.jsonl (deflated 77%)\n",
            "  adding: content/governmentgpt/tb/ (stored 0%)\n",
            "  adding: content/governmentgpt/tb/events.out.tfevents.1719405135.3c99aeb64f5e.6250.1.eval (deflated 8%)\n",
            "  adding: content/governmentgpt/tb/events.out.tfevents.1719405135.3c99aeb64f5e.6250.0.train (deflated 73%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "eiIyRfRh5IYX",
        "outputId": "5425a00f-c8e2-48cd-cb59-ceacd2cba105"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-4996ee3d8d09>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         )\n\u001b[0;32m--> 283\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm /content/gdrive/MyDrive/governmentgpt.zip\n",
        "!mv /content/governmentgpt.zip /content/gdrive/MyDrive/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixJULjHo7TLc",
        "outputId": "bdf22323-75e4-4c52-a9da-f51273c66490"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/gdrive/MyDrive/governmentgpt.zip': No such file or directory\n",
            "mv: cannot move '/content/governmentgpt.zip' to '/content/gdrive/MyDrive/': No such file or directory\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "973c4bb71e3544ad973420c227d16947": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ab530c0b4ad9413d85ccb26847c6c9e9",
              "IPY_MODEL_70188b6e3bd74364a7ee078f93cff20d",
              "IPY_MODEL_74240e21e41b4c52aa70b69d1df9fdea"
            ],
            "layout": "IPY_MODEL_54b6bf2b07904fd99bb657abe69c0227"
          }
        },
        "ab530c0b4ad9413d85ccb26847c6c9e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9593542b26c499ba1b08abe607207eb",
            "placeholder": "​",
            "style": "IPY_MODEL_065ebf7c80e8421d8c272a682e6a5dc4",
            "value": "Fetching 3 files: 100%"
          }
        },
        "70188b6e3bd74364a7ee078f93cff20d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b07209006a7426aba2640b0349fa55d",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ebdfe4731ea54a679a587cacc754cba3",
            "value": 3
          }
        },
        "74240e21e41b4c52aa70b69d1df9fdea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75885cfae9b94b2295bed7d91ff90437",
            "placeholder": "​",
            "style": "IPY_MODEL_fcea1cd754e6416489b9db6cb6996aff",
            "value": " 3/3 [01:18&lt;00:00, 78.12s/it]"
          }
        },
        "54b6bf2b07904fd99bb657abe69c0227": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9593542b26c499ba1b08abe607207eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "065ebf7c80e8421d8c272a682e6a5dc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b07209006a7426aba2640b0349fa55d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebdfe4731ea54a679a587cacc754cba3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "75885cfae9b94b2295bed7d91ff90437": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcea1cd754e6416489b9db6cb6996aff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3893d50398544a46bcf3e4ae0b7290ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9f44dedd590246abb6ca5dc21771f42b",
              "IPY_MODEL_44b4745d2b45465b8b1f412224e57dba",
              "IPY_MODEL_cba14c8db6e8435d9bd67a2de1f51d29"
            ],
            "layout": "IPY_MODEL_2d515e5acfe24ab1945ffbd76d2488c8"
          }
        },
        "9f44dedd590246abb6ca5dc21771f42b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc3fc9cc3be3453396bd1d59db8ad675",
            "placeholder": "​",
            "style": "IPY_MODEL_a670a9bd66fa4e69a484902ec39fb044",
            "value": "tokenizer.model.v3: 100%"
          }
        },
        "44b4745d2b45465b8b1f412224e57dba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70546d1d5c4b42088d6fbea2972f7aa5",
            "max": 587404,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_afe31d4282cf44afb95273d3b95f3fed",
            "value": 587404
          }
        },
        "cba14c8db6e8435d9bd67a2de1f51d29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c043b5888f144945be2da6512e798ec0",
            "placeholder": "​",
            "style": "IPY_MODEL_1a7928bb7a894f5cb0ae0f726d89de0f",
            "value": " 587k/587k [00:00&lt;00:00, 5.86MB/s]"
          }
        },
        "2d515e5acfe24ab1945ffbd76d2488c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc3fc9cc3be3453396bd1d59db8ad675": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a670a9bd66fa4e69a484902ec39fb044": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70546d1d5c4b42088d6fbea2972f7aa5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afe31d4282cf44afb95273d3b95f3fed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c043b5888f144945be2da6512e798ec0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a7928bb7a894f5cb0ae0f726d89de0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05e575b33c614951b1cde79860bcdfc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1a7810fecfde4b7b87430f4b2a86de86",
              "IPY_MODEL_810e9d9bcf574cbb82c83a34445df56b",
              "IPY_MODEL_76ceb6c3812347f3a7e873acef9b33ff"
            ],
            "layout": "IPY_MODEL_3c26a3fe6ddc46aaa7ae817b83f0cf25"
          }
        },
        "1a7810fecfde4b7b87430f4b2a86de86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24b023d1fb5f47d4b382ba15be216ac1",
            "placeholder": "​",
            "style": "IPY_MODEL_3de279dcc04a4d35bd61e3d6ec32399a",
            "value": "params.json: 100%"
          }
        },
        "810e9d9bcf574cbb82c83a34445df56b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_625a865cc43a45028fb818acc986a216",
            "max": 202,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2e627687b8454e12a3a0d60d317bf362",
            "value": 202
          }
        },
        "76ceb6c3812347f3a7e873acef9b33ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c901cfec744c42a0a204c54c3a2ab4ef",
            "placeholder": "​",
            "style": "IPY_MODEL_9d634e0998d5467ead1ccf940bdad0d0",
            "value": " 202/202 [00:00&lt;00:00, 16.5kB/s]"
          }
        },
        "3c26a3fe6ddc46aaa7ae817b83f0cf25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24b023d1fb5f47d4b382ba15be216ac1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3de279dcc04a4d35bd61e3d6ec32399a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "625a865cc43a45028fb818acc986a216": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e627687b8454e12a3a0d60d317bf362": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c901cfec744c42a0a204c54c3a2ab4ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d634e0998d5467ead1ccf940bdad0d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fba8f0da949a4feda5be6fe27f6a5437": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a495b56297eb4f6ea2cbeaadceb05d5c",
              "IPY_MODEL_4e6ee6e0bb534e67a6661f6286144549",
              "IPY_MODEL_dd9b83a9288d473c8633249dc9ff9696"
            ],
            "layout": "IPY_MODEL_1cf226bc325943848221e78913309eaa"
          }
        },
        "a495b56297eb4f6ea2cbeaadceb05d5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33d177d4058b409a8b6180a2cf4bbb03",
            "placeholder": "​",
            "style": "IPY_MODEL_1746029a40ac4a1395cdac05c9b0d229",
            "value": "consolidated.safetensors: 100%"
          }
        },
        "4e6ee6e0bb534e67a6661f6286144549": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de9654df2a5c44428ee00c0fa42c0073",
            "max": 14496078512,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_de636eb04bc44e718c2f77c74e9c4e99",
            "value": 14496078512
          }
        },
        "dd9b83a9288d473c8633249dc9ff9696": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fde0c9d111064c76a197f990d40e5b4b",
            "placeholder": "​",
            "style": "IPY_MODEL_e69a3182bd7145229f9b340fffecbd4e",
            "value": " 14.5G/14.5G [01:17&lt;00:00, 185MB/s]"
          }
        },
        "1cf226bc325943848221e78913309eaa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33d177d4058b409a8b6180a2cf4bbb03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1746029a40ac4a1395cdac05c9b0d229": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de9654df2a5c44428ee00c0fa42c0073": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de636eb04bc44e718c2f77c74e9c4e99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fde0c9d111064c76a197f990d40e5b4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e69a3182bd7145229f9b340fffecbd4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}